{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_type='bert-base-uncased'\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')  \n",
    "# bert = transformers.BertModel.from_pretrained(context_type, config=config)\n",
    "# bert = bert.to('cuda:0')\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(context_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBert(nn.Module):\n",
    "    def __init__(self, context_type, max_length):\n",
    "        super(MyBert, self).__init__()\n",
    "        config = transformers.BertConfig.from_pretrained('bert-base-uncased')  \n",
    "        bert = transformers.BertModel.from_pretrained(context_type, config=config)\n",
    "        tokenizer = transformers.BertTokenizer.from_pretrained(context_type)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = tokenizer(\n",
    "            text=tmp_text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation='longest_first',\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        out = bert(**x, return_dict=True)\n",
    "        return out.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bert = MyBert(context_type, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_text = 'guideline hand hygiene health care setting recommendation healthcare infection control practice advisory committee hicpac shea ape idsa hand hygiene task force prepared john boyce md didier pittet md hospital saint raphael new haven connecticut university geneva geneva switzerland material report originate national center infectious disease james hughes md director division healthcare quality promotion steve solomon md acting director summary guideline hand hygiene health care setting health care worker hcw review data regarding handwash hand antisepsi health care setting addition specific recommendation promote improve hand hygiene practice reduce transmission pathogenic microorganism patient personnel health care setting report review study publish cdc guideline garner js favero cdc guideline handwash hospital environmental control infect control ape guideline larson el ape guidelines committee ape guideline handwash hand antisepsi health care setting infect control issue depth review hand hygiene practice hcw level adherence personnel recommend handwash practice factor adverse affecting adherence new study vivo efficacy alcohol base hand rub low incidence dermatitis associate use review recent study demonstrate value multidisciplinary hand hygiene promotion program potential role alcohol base hand rub improve hand hygiene practice summarize recommendation concerning related issue e use surgical hand antiseptic hand lotion cream wearing artificial fingernail part review scientific data regarding hand hygiene guideline hand hygiene health care setting recommendation healthcare infection control practice advisory committee hicpac shea ape idsa hand hygiene task force prepared john boyce md didier pittet md hospital saint raphael new haven connecticut university geneva geneva switzerland material report originate national center infectious disease james hughes md director division healthcare quality promotion steve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tokenizer(\n",
    "    text=tmp_text, \n",
    "    return_tensors=\"pt\", \n",
    "    truncation='longest_first',\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    ")\n",
    "dummy_input = dummy_input.to(bert.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = (dummy_input['input_ids'], dummy_input['token_type_ids'], dummy_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5009,  4179,  2192, 19548,  2740,  2729,  4292, 12832,  9871,\n",
       "          8985,  2491,  3218,  7319,  2837,  7632, 21906,  6305, 16994, 23957,\n",
       "          8909,  3736,  2192, 19548,  4708,  2486,  4810,  2198,  2879,  3401,\n",
       "          9108,  2106,  3771, 15091,  3388,  9108,  2902,  3002, 12551,  2047,\n",
       "          4033,  6117,  2118,  9810,  9810,  5288,  3430,  3189, 21754,  2120,\n",
       "          2415, 16514,  4295,  2508,  8099,  9108,  2472,  2407,  9871,  3737,\n",
       "          4712,  3889,  9168,  9108,  3772,  2472, 12654,  5009,  4179,  2192,\n",
       "         19548,  2740,  2729,  4292,  2740,  2729,  7309, 16731,  2860,  3319,\n",
       "          2951,  4953,  2192, 28556,  2192,  3424,  3366,  4523,  2072,  2740,\n",
       "          2729,  4292,  2804,  3563, 12832,  5326,  5335,  2192, 19548,  3218,\n",
       "          5547,  6726, 26835,  2594, 12702, 21759,  7088,  6491,  5776,  5073,\n",
       "          2740,  2729,  4292,  3189,  3319,  2817, 10172, 26629,  5009,  4179,\n",
       "         18661,  1046,  2015,  6904,  6299,  2080, 26629,  5009,  4179,  2192,\n",
       "         28556,  2902,  4483,  2491,  1999, 25969,  2491, 23957,  5009,  4179,\n",
       "         21213,  3449, 23957, 11594,  2837, 23957,  5009,  4179,  2192, 28556,\n",
       "          2192,  3424,  3366,  4523,  2072,  2740,  2729,  4292,  1999, 25969,\n",
       "          2491,  3277,  5995,  3319,  2192, 19548,  3218, 16731,  2860,  2504,\n",
       "         29235,  5073, 16755,  2192, 28556,  3218,  5387, 15316, 12473, 29235,\n",
       "          2047,  2817, 24269, 21150,  6544,  2918,  2192, 14548,  2659, 18949,\n",
       "          4315, 18900, 13706,  5482,  2224,  3319,  3522,  2817, 10580,  3643,\n",
       "          4800, 10521,  6895, 28296,  5649,  2192, 19548,  4712,  2565,  4022,\n",
       "          2535,  6544,  2918,  2192, 14548,  5335,  2192, 19548,  3218,  7680,\n",
       "          7849,  4697, 12832,  7175,  3141,  3277,  1041,  2224, 11707,  2192,\n",
       "          3424,  3366, 20746,  2192,  2843,  3258,  6949,  4147,  7976,  4344,\n",
       "         25464,  2112,  3319,  4045,  2951,  4953,  2192, 19548,  5009,  4179,\n",
       "          2192, 19548,  2740,  2729,  4292,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(bert, dummy_input, \"./bert.onnx\", opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /home/ninglu_shao/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe42fa750ee46f4b73a55723bbd2c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "import torchvision\n",
    "\n",
    "# Standard ImageNet input - 3 channels, 224x224,\n",
    "# values don't matter as we care about network structure.\n",
    "# But they can also be real inputs.\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Obtain your model, it can be also constructed in your script explicitly\n",
    "model = torchvision.models.alexnet(pretrained=True)\n",
    "# Invoke export\n",
    "torch.onnx.export(model, dummy_input, \"alexnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: bert-base-uncased, tokenizer: bert-base-uncased)\n",
      "Using framework PyTorch: 1.8.0\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin_bo/anaconda3/envs/torch_test/lib/python3.8/site-packages/transformers/modeling_utils.py:1669: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "# Handles all the above steps for you\n",
    "convert(framework=\"pt\", model=\"bert-base-uncased\", output=Path('onnx/a.onnx'), opset=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "\n",
    "# Constants from the performance optimization available in onnxruntime\n",
    "# It needs to be done before importing onnxruntime\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True)) # OMP 的线程数\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\n",
    "\n",
    "\n",
    "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
    "  \n",
    "    assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "\n",
    "    # Few properties that might have an impact on performances (provided by MS)\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "  # Load the model as a graph and prepare the CPU backend \n",
    "    session = InferenceSession(model_path, options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcublas.so.10.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f94f2b18f036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/new_disk1/ninglu_shao/anaconda3/envs/nn/lib/python3.7/site-packages/onnxruntime/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Microsoft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybind_state\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_all_providers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_available_providers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSessionOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_default_logger_severity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNodeArg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelMetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphOptimizationLevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExecutionMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferenceSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/new_disk1/ninglu_shao/anaconda3/envs/nn/lib/python3.7/site-packages/onnxruntime/capi/_pybind_state.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ld_preload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/new_disk1/ninglu_shao/anaconda3/envs/nn/lib/python3.7/site-packages/onnxruntime/capi/_ld_preload.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# LD_PRELOAD_BEGIN_MARK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mctypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0m_libcublas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libcublas.so.10.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0m_libcudnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libcudnn.so.7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0m_libcudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libcudart.so.10.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/new_disk1/ninglu_shao/anaconda3/envs/nn/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: libcublas.so.10.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'onnx/a.onnx'\n",
    "options = onnxruntime.SessionOptions()\n",
    "session = onnxruntime.InferenceSession(model_path, options)\n",
    "model_inputs = tokenizer(tmp_text, \n",
    "                         return_tensors=\"pt\",\n",
    "                         truncation='longest_first',\n",
    "                         padding='max_length',\n",
    "                         max_length=max_length,)\n",
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "sequence, pooled = session.run(None, inputs_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence output: (1, 256, 768), Pooled output: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "cpu_model = create_model_for_provider(\"onnx/a.onnx\", \"CPUExecutionProvider\") # 使用 优化过的 onnx\n",
    "\n",
    "# Inputs are provided through numpy array\n",
    "model_inputs = tokenizer(tmp_text, \n",
    "                         return_tensors=\"pt\",\n",
    "                         truncation='longest_first',\n",
    "                         padding='max_length',\n",
    "                         max_length=max_length,)\n",
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "\n",
    "# Run the model (None = get all the outputs)\n",
    "sequence, pooled = cpu_model.run(None, inputs_onnx)\n",
    "\n",
    "# Print information about outputs\n",
    "print(f\"Sequence output: {sequence.shape}, Pooled output: {pooled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7876456 , -0.37514284, -0.62177825,  0.51467806,  0.04451199,\n",
       "        -0.06266444,  0.542937  ,  0.32615924, -0.22711486, -0.999966  ,\n",
       "        -0.0253908 ,  0.66451746,  0.9587116 ,  0.26778808,  0.7770741 ,\n",
       "        -0.46493056,  0.32824275, -0.47594348,  0.21665512,  0.16078912,\n",
       "         0.49847522,  0.9990304 ,  0.21279846,  0.41808248,  0.33688176,\n",
       "         0.5829688 , -0.40536615,  0.78604555,  0.87679315,  0.65466446,\n",
       "        -0.45456868,  0.34951618, -0.951446  , -0.15150009, -0.6541763 ,\n",
       "        -0.98015136,  0.21875226, -0.53091437, -0.03381576,  0.00569992,\n",
       "        -0.6860586 ,  0.34882244,  0.9997967 , -0.20320052,  0.11019732,\n",
       "        -0.26156852, -0.9999467 ,  0.20039761, -0.67345405,  0.43705562,\n",
       "         0.2182735 ,  0.46409705,  0.14355789,  0.29323584,  0.33562776,\n",
       "         0.09232467, -0.10981961,  0.08744046, -0.2797671 , -0.46606663,\n",
       "        -0.578237  ,  0.4825899 , -0.5072    , -0.8069033 ,  0.03463387,\n",
       "         0.33685806, -0.21042414, -0.33315817, -0.09000603, -0.0392775 ,\n",
       "         0.67170626,  0.1863854 , -0.29178804, -0.67997414,  0.14340618,\n",
       "         0.18772706, -0.68969274,  1.        , -0.21291675, -0.9034928 ,\n",
       "         0.5860319 ,  0.55030364,  0.57692355, -0.00599359,  0.06583592,\n",
       "        -0.99999917,  0.34427246, -0.12411468, -0.9685128 ,  0.29744393,\n",
       "         0.29985473, -0.1274716 ,  0.42910394,  0.6601146 , -0.44667178,\n",
       "        -0.29498705, -0.16289212, -0.19694978, -0.2722491 , -0.19005476,\n",
       "         0.10470243, -0.09802333, -0.21564972, -0.31579342,  0.18267477,\n",
       "        -0.36806104, -0.1539979 ,  0.39889908,  0.1521775 ,  0.5510242 ,\n",
       "         0.5562555 , -0.3650104 ,  0.18856701, -0.8342922 ,  0.514788  ,\n",
       "        -0.23912118, -0.9440891 , -0.63152975, -0.9635664 ,  0.5625529 ,\n",
       "         0.11903129, -0.16774024,  0.821608  ,  0.0150331 ,  0.35109138,\n",
       "         0.02184083, -0.43750712, -1.        , -0.39940923, -0.12205392,\n",
       "        -0.00229575, -0.09084895, -0.9197147 , -0.8559353 ,  0.50655943,\n",
       "         0.8434786 ,  0.1390434 ,  0.9992325 , -0.13122945,  0.8396426 ,\n",
       "        -0.02097198, -0.41947174, -0.14980857, -0.4136141 ,  0.4650624 ,\n",
       "        -0.13930465, -0.35714966,  0.19360754, -0.16279542,  0.1102321 ,\n",
       "        -0.3370644 , -0.26399624, -0.23121394, -0.73352605, -0.34238645,\n",
       "         0.8618284 ,  0.09707064, -0.556756  ,  0.30003542, -0.22712545,\n",
       "        -0.2034025 ,  0.718549  ,  0.2654945 ,  0.20078333, -0.15149146,\n",
       "         0.33884066,  0.17799495,  0.3493274 , -0.68621767, -0.01008895,\n",
       "         0.25260487, -0.2628726 , -0.41944274, -0.9355184 , -0.34399733,\n",
       "         0.37905464,  0.9287507 ,  0.5353317 ,  0.16553214,  0.40883926,\n",
       "        -0.3730023 ,  0.4543953 , -0.8915117 ,  0.93563217,  0.03157876,\n",
       "         0.21753486,  0.11743863,  0.2892985 , -0.53486055,  0.03808994,\n",
       "         0.7298571 , -0.258978  , -0.6550359 ,  0.00328219, -0.43552265,\n",
       "        -0.44678468, -0.32731134,  0.36587635, -0.18235445, -0.33033648,\n",
       "        -0.04792338,  0.7959511 ,  0.88115287,  0.396862  , -0.09282064,\n",
       "         0.44483155, -0.801696  , -0.25021178,  0.14220935,  0.17035948,\n",
       "        -0.05622571,  0.9673828 , -0.5023424 ,  0.07358504, -0.6018953 ,\n",
       "        -0.9506653 ,  0.02332556, -0.58683074, -0.05022218, -0.5345163 ,\n",
       "         0.43815956,  0.2432084 ,  0.08490832,  0.20563562, -0.7258667 ,\n",
       "        -0.5085089 ,  0.13194583, -0.31962568,  0.36657247, -0.21288408,\n",
       "         0.63725036,  0.68920326, -0.5651169 ,  0.01531416,  0.8974215 ,\n",
       "        -0.49521467, -0.5085099 ,  0.49384794, -0.14037138,  0.73619753,\n",
       "        -0.56440073,  0.9862271 ,  0.55230093,  0.2389901 , -0.79112744,\n",
       "        -0.45264587, -0.585136  , -0.30171677, -0.03661821, -0.22055523,\n",
       "         0.43664286,  0.64444697,  0.23637302,  0.3800055 , -0.38274515,\n",
       "         0.87776965, -0.3481552 , -0.8844323 , -0.35307327, -0.09285246,\n",
       "        -0.960783  ,  0.36163735,  0.2975532 , -0.05026329, -0.34636426,\n",
       "        -0.44147828, -0.80385697,  0.5849662 ,  0.05758233,  0.8968321 ,\n",
       "        -0.05948463, -0.7528517 , -0.42851624, -0.7817108 , -0.10597312,\n",
       "        -0.07062619,  0.15098104, -0.05540731, -0.7765742 ,  0.43607277,\n",
       "         0.4027932 ,  0.3128677 , -0.3670611 ,  0.94303733,  0.99996275,\n",
       "         0.9113186 ,  0.71742296,  0.55305934, -0.9932389 , -0.1165138 ,\n",
       "         0.9999906 , -0.8466358 , -0.99999774, -0.77684027, -0.55697006,\n",
       "         0.12478419, -1.        , -0.11924251,  0.02145816, -0.6756374 ,\n",
       "         0.17885527,  0.93022794,  0.85057604, -0.9999998 ,  0.80557543,\n",
       "         0.7922432 , -0.7214485 ,  0.31737363, -0.23842274,  0.9327981 ,\n",
       "         0.18817705,  0.5948914 , -0.23371005,  0.42221165, -0.6810079 ,\n",
       "        -0.6984164 ,  0.09709995, -0.05121924,  0.9674158 ,  0.19825292,\n",
       "        -0.73549485, -0.71315175, -0.09252422,  0.10637952, -0.21007134,\n",
       "        -0.87074906, -0.2056531 , -0.27220425,  0.47109675,  0.06640638,\n",
       "         0.16079892, -0.52930635,  0.13684343, -0.18456466,  0.20781383,\n",
       "         0.6692587 , -0.9069945 , -0.40511137, -0.04482692, -0.14933524,\n",
       "        -0.35214743, -0.93430877,  0.924726  , -0.219945  ,  0.4837175 ,\n",
       "         1.        ,  0.07714168, -0.6575619 ,  0.30535617, -0.00680418,\n",
       "        -0.23511161,  0.99999964,  0.5801956 , -0.9455009 , -0.65123355,\n",
       "         0.51135254, -0.42921418, -0.44299895,  0.99925375, -0.12194716,\n",
       "        -0.13024855,  0.18664683,  0.9529362 , -0.9722437 ,  0.93386716,\n",
       "        -0.76493263, -0.8426922 ,  0.856395  ,  0.74894214, -0.20455803,\n",
       "        -0.58996725,  0.1170795 , -0.5668946 ,  0.26051167, -0.77159953,\n",
       "         0.42457867,  0.32734415, -0.02862913,  0.69190073, -0.38229966,\n",
       "        -0.5802953 ,  0.36768708, -0.21186642, -0.10418536,  0.59605086,\n",
       "         0.36758053, -0.12850955,  0.03046969, -0.22152855, -0.18571456,\n",
       "        -0.9150426 ,  0.24916969,  0.99999964, -0.19593608, -0.15786155,\n",
       "        -0.38674873,  0.06800088, -0.21166329,  0.35510722,  0.4101905 ,\n",
       "        -0.17558095, -0.704218  ,  0.00747266, -0.5931253 , -0.9691983 ,\n",
       "         0.31216425,  0.08067415, -0.19793744,  0.9992689 ,  0.16404963,\n",
       "         0.19540122,  0.14872505,  0.63802344,  0.11336372,  0.2168916 ,\n",
       "         0.26148084,  0.90523905, -0.32444382,  0.65486693,  0.52697307,\n",
       "        -0.4261662 , -0.18628438, -0.49873096, -0.00330743, -0.8541671 ,\n",
       "         0.07141053, -0.83243096,  0.8415314 ,  0.55832523,  0.23398976,\n",
       "         0.20235354,  0.32492927,  0.9999999 , -0.5703202 ,  0.40929836,\n",
       "         0.29530233,  0.3421208 , -0.9892111 , -0.49029547, -0.3130371 ,\n",
       "        -0.01695211, -0.23167719, -0.20114242,  0.00791435, -0.9126609 ,\n",
       "         0.3496346 , -0.07098618, -0.77703327, -0.95450085, -0.16252707,\n",
       "         0.48837298,  0.03055358, -0.7806984 , -0.43326685, -0.44882587,\n",
       "         0.27791098, -0.291146  , -0.8088856 ,  0.18486853, -0.16587907,\n",
       "         0.3632235 , -0.22867386,  0.607654  ,  0.36415026,  0.6405047 ,\n",
       "        -0.21824831, -0.29909492, -0.14355652, -0.71667534,  0.31460026,\n",
       "        -0.57378846, -0.20562996, -0.1135138 ,  1.        ,  0.06006809,\n",
       "         0.56629086,  0.39284983,  0.2968141 , -0.2762671 ,  0.13383494,\n",
       "         0.65215635,  0.18566284, -0.31693107, -0.18431555,  0.40331984,\n",
       "        -0.36205578,  0.5775363 , -0.08713719,  0.4148551 ,  0.5395329 ,\n",
       "         0.39846724,  0.21198827,  0.03057607,  0.06605463,  0.9432659 ,\n",
       "        -0.21749818, -0.08310501, -0.31057382, -0.11936364, -0.28312185,\n",
       "         0.14163747,  0.9999991 ,  0.16691945,  0.388807  , -0.97446465,\n",
       "        -0.3482935 , -0.80182105,  0.99988574,  0.5995122 , -0.6817078 ,\n",
       "         0.41900113,  0.34083787, -0.10685797,  0.24155146, -0.18428257,\n",
       "        -0.20110314,  0.21687587,  0.09463   ,  0.85393757, -0.3704847 ,\n",
       "        -0.89777714, -0.5853286 ,  0.3461459 , -0.8209889 ,  0.9971738 ,\n",
       "        -0.5742754 , -0.10187295, -0.16763806, -0.01400408, -0.31722346,\n",
       "        -0.10485366, -0.9295393 , -0.12478584, -0.04934717,  0.89311653,\n",
       "         0.12411945, -0.67942953, -0.8589577 ,  0.29125568,  0.4426364 ,\n",
       "        -0.2592665 , -0.79799485,  0.8821011 , -0.8899557 ,  0.20434044,\n",
       "         0.99999344,  0.39636418,  0.02129158,  0.22386532, -0.18611346,\n",
       "         0.27606517,  0.13268918,  0.3228512 , -0.74577945, -0.19778769,\n",
       "        -0.12747629,  0.29420793, -0.09727805, -0.38711175,  0.39583626,\n",
       "         0.16139339, -0.5963769 , -0.5605243 , -0.03191755,  0.3108306 ,\n",
       "         0.5460953 , -0.27341464, -0.09570954,  0.07725237, -0.04774416,\n",
       "        -0.829681  , -0.31420374, -0.22569649, -0.9990595 ,  0.5323165 ,\n",
       "        -0.99999994,  0.06499074,  0.07948671, -0.16290389,  0.65260917,\n",
       "         0.13305508,  0.12022618, -0.26662984, -0.22885612,  0.5500633 ,\n",
       "         0.61001265, -0.08727211,  0.00572416, -0.2662598 ,  0.19644473,\n",
       "        -0.06154927,  0.15035334, -0.24135487,  0.5304824 , -0.16760784,\n",
       "         1.        ,  0.05220586, -0.31528044, -0.6250835 ,  0.26606762,\n",
       "        -0.11090552,  0.999988  , -0.50756824, -0.8707813 ,  0.2906062 ,\n",
       "        -0.48204678, -0.52149117,  0.10963473,  0.09351226, -0.7015952 ,\n",
       "        -0.4971331 ,  0.8332693 ,  0.55864304, -0.6496956 ,  0.20609446,\n",
       "        -0.31807348, -0.33467525,  0.04713855,  0.3995587 ,  0.95932263,\n",
       "         0.0752712 ,  0.63570005,  0.22748487,  0.15535745,  0.857872  ,\n",
       "         0.26821   ,  0.03980742,  0.07159216,  0.99999833,  0.2697273 ,\n",
       "        -0.8611572 ,  0.25252444, -0.8851421 , -0.24678726, -0.81664485,\n",
       "         0.20479217,  0.1552335 ,  0.651956  , -0.16959171,  0.8678802 ,\n",
       "        -0.29978347, -0.077521  ,  0.15308605, -0.05442965,  0.3655505 ,\n",
       "        -0.845599  , -0.96545917, -0.95911926,  0.29106098, -0.4373674 ,\n",
       "        -0.02327895,  0.23281144,  0.14666325,  0.26081246,  0.320688  ,\n",
       "        -0.9999985 ,  0.8420244 ,  0.42088264,  0.46005595,  0.8481581 ,\n",
       "         0.32254702,  0.3624636 ,  0.24234037, -0.9499957 , -0.5076692 ,\n",
       "        -0.35883212, -0.33283564,  0.5336804 ,  0.557876  ,  0.49065265,\n",
       "         0.26098084, -0.285537  , -0.2293853 , -0.1921823 , -0.30408734,\n",
       "        -0.96664095,  0.41850838, -0.2858762 , -0.65507954,  0.8761345 ,\n",
       "        -0.05962191, -0.047676  ,  0.28816214, -0.57342553,  0.7511137 ,\n",
       "         0.70179725,  0.09291661, -0.0158473 ,  0.4148362 ,  0.70250696,\n",
       "         0.8458    ,  0.9726921 , -0.58026433,  0.5222352 , -0.21783671,\n",
       "         0.28148916,  0.49828783, -0.8490096 ,  0.17260638,  0.12021212,\n",
       "        -0.20642109,  0.29127362, -0.2552996 , -0.7423616 ,  0.5465753 ,\n",
       "        -0.36260578,  0.41783795, -0.34774086,  0.14310206, -0.36863422,\n",
       "        -0.16330118, -0.52970254, -0.36694756,  0.6838857 ,  0.05340698,\n",
       "         0.8352327 ,  0.4840644 , -0.0456544 , -0.41321105, -0.19097948,\n",
       "        -0.34357494, -0.81613106,  0.65594447,  0.02247219, -0.2644502 ,\n",
       "         0.15634735, -0.16818532,  0.8081746 ,  0.02734569, -0.25475255,\n",
       "        -0.20170362, -0.65081453,  0.63640535, -0.41983292, -0.4422731 ,\n",
       "        -0.26371536,  0.54030854,  0.2697933 ,  0.9983583 , -0.37425664,\n",
       "        -0.49445686, -0.28570023, -0.16945241,  0.24123143, -0.26384255,\n",
       "        -0.9999988 ,  0.12291241, -0.3749102 ,  0.33875385,  0.13649103,\n",
       "         0.37726167, -0.43390495, -0.87169766, -0.17207764,  0.1991512 ,\n",
       "         0.16108422, -0.46209282, -0.41617826,  0.6861692 ,  0.03172074,\n",
       "         0.47555953,  0.61163735, -0.15718313,  0.03933119,  0.6864634 ,\n",
       "        -0.47156608, -0.56570035,  0.61748075]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.7 s, sys: 208 ms, total: 44.9 s\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1):\n",
    "    out = cpu_model.run(None, inputs_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(\"A dog is running on beach\", \n",
    "                         return_tensors=\"pt\",\n",
    "                         truncation='longest_first',\n",
    "                         padding='max_length',\n",
    "                         max_length=max_length,)\n",
    "model_inputs = model_inputs.to(bert.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(\"A dog is running on beach\", \n",
    "                         return_tensors=\"pt\")\n",
    "model_inputs = model_inputs.to(bert.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(tmp_text, \n",
    "                         return_tensors=\"pt\",\n",
    "                         truncation='longest_first',\n",
    "                         padding='max_length',\n",
    "                         max_length=max_length,)\n",
    "model_inputs = model_inputs.to(bert.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5009,  4179,  2192, 19548,  2740,  2729,  4292, 12832,  9871,\n",
       "          8985,  2491,  3218,  7319,  2837,  7632, 21906,  6305, 16994, 23957,\n",
       "          8909,  3736,  2192, 19548,  4708,  2486,  4810,  2198,  2879,  3401,\n",
       "          9108,  2106,  3771, 15091,  3388,  9108,  2902,  3002, 12551,  2047,\n",
       "          4033,  6117,  2118,  9810,  9810,  5288,  3430,  3189, 21754,  2120,\n",
       "          2415, 16514,  4295,  2508,  8099,  9108,  2472,  2407,  9871,  3737,\n",
       "          4712,  3889,  9168,  9108,  3772,  2472, 12654,  5009,  4179,  2192,\n",
       "         19548,  2740,  2729,  4292,  2740,  2729,  7309, 16731,  2860,  3319,\n",
       "          2951,  4953,  2192, 28556,  2192,  3424,  3366,  4523,  2072,  2740,\n",
       "          2729,  4292,  2804,  3563, 12832,  5326,  5335,  2192, 19548,  3218,\n",
       "          5547,  6726, 26835,  2594, 12702, 21759,  7088,  6491,  5776,  5073,\n",
       "          2740,  2729,  4292,  3189,  3319,  2817, 10172, 26629,  5009,  4179,\n",
       "         18661,  1046,  2015,  6904,  6299,  2080, 26629,  5009,  4179,  2192,\n",
       "         28556,  2902,  4483,  2491,  1999, 25969,  2491, 23957,  5009,  4179,\n",
       "         21213,  3449, 23957, 11594,  2837, 23957,  5009,  4179,  2192, 28556,\n",
       "          2192,  3424,  3366,  4523,  2072,  2740,  2729,  4292,  1999, 25969,\n",
       "          2491,  3277,  5995,  3319,  2192, 19548,  3218, 16731,  2860,  2504,\n",
       "         29235,  5073, 16755,  2192, 28556,  3218,  5387, 15316, 12473, 29235,\n",
       "          2047,  2817, 24269, 21150,  6544,  2918,  2192, 14548,  2659, 18949,\n",
       "          4315, 18900, 13706,  5482,  2224,  3319,  3522,  2817, 10580,  3643,\n",
       "          4800, 10521,  6895, 28296,  5649,  2192, 19548,  4712,  2565,  4022,\n",
       "          2535,  6544,  2918,  2192, 14548,  5335,  2192, 19548,  3218,  7680,\n",
       "          7849,  4697, 12832,  7175,  3141,  3277,  1041,  2224, 11707,  2192,\n",
       "          3424,  3366, 20746,  2192,  2843,  3258,  6949,  4147,  7976,  4344,\n",
       "         25464,  2112,  3319,  4045,  2951,  4953,  2192, 19548,  5009,  4179,\n",
       "          2192, 19548,  2740,  2729,  4292,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['input_ids'] = model_inputs['input_ids'].int()\n",
    "model_inputs['token_type_ids'] = model_inputs['token_type_ids'].int()\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.IntTensor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 1037, 3899, 2003, 2770, 2006, 3509,  102]]),\n",
       " 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "inputs_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.45129588,  0.17464024, -0.32815534, ..., -0.34352878,\n",
       "          0.95904636, -0.3726815 ],\n",
       "        [-0.02382827,  0.18869871, -0.14093272, ..., -0.22917065,\n",
       "          1.0786096 , -0.26578447],\n",
       "        [-0.3177335 ,  0.67381006,  0.25352606, ..., -0.2675582 ,\n",
       "          0.41499317, -0.7243721 ],\n",
       "        ...,\n",
       "        [-0.10590474,  0.263191  ,  0.12382512, ..., -0.5368702 ,\n",
       "          0.45611474, -0.33608583],\n",
       "        [ 0.21408592, -0.13089575,  0.04547495, ..., -0.42025897,\n",
       "          0.41084263, -0.9893264 ],\n",
       "        [ 0.16522332,  0.10745797, -0.46665922, ...,  0.02295056,\n",
       "         -0.15455595, -0.34739056]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 979 ms, total: 4 s\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1):  \n",
    "    out = bert(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42min 41s, sys: 29.9 s, total: 43min 11s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1000):  \n",
    "    out = bert(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 ,1037 ,3899 ,2003 ,2770 ,2006 ,3509 ,102 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,"
     ]
    }
   ],
   "source": [
    "for i in range(256):\n",
    "    print(model_inputs['input_ids'][0][i].item(), ',', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3401,  0.4414, -0.0335,  ..., -0.3966,  0.1183,  0.2993],\n",
       "          [ 0.4137, -0.2426,  0.7700,  ..., -0.0194,  0.4660, -0.0095],\n",
       "          [-0.4527, -0.0234,  0.5384,  ..., -0.3647, -0.3127, -1.1680],\n",
       "          ...,\n",
       "          [-0.1401, -0.5340,  1.0292,  ..., -0.3733, -0.3382, -0.3510],\n",
       "          [ 0.1625,  0.1707, -0.0268,  ..., -0.6657, -0.6873, -0.4199],\n",
       "          [ 0.3701,  0.4741, -0.3531,  ..., -0.0108, -0.0713,  0.1101]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[-0.7876, -0.3751, -0.6218,  0.5147,  0.0445, -0.0627,  0.5429,  0.3262,\n",
       "          -0.2271, -1.0000, -0.0254,  0.6645,  0.9587,  0.2678,  0.7771, -0.4649,\n",
       "           0.3282, -0.4759,  0.2167,  0.1608,  0.4985,  0.9990,  0.2128,  0.4181,\n",
       "           0.3369,  0.5830, -0.4054,  0.7860,  0.8768,  0.6547, -0.4546,  0.3495,\n",
       "          -0.9514, -0.1515, -0.6542, -0.9802,  0.2188, -0.5309, -0.0338,  0.0057,\n",
       "          -0.6861,  0.3488,  0.9998, -0.2032,  0.1102, -0.2616, -0.9999,  0.2004,\n",
       "          -0.6735,  0.4371,  0.2183,  0.4641,  0.1436,  0.2932,  0.3356,  0.0923,\n",
       "          -0.1098,  0.0874, -0.2798, -0.4661, -0.5782,  0.4826, -0.5072, -0.8069,\n",
       "           0.0346,  0.3369, -0.2104, -0.3332, -0.0900, -0.0393,  0.6717,  0.1864,\n",
       "          -0.2918, -0.6800,  0.1434,  0.1877, -0.6897,  1.0000, -0.2129, -0.9035,\n",
       "           0.5860,  0.5503,  0.5769, -0.0060,  0.0658, -1.0000,  0.3443, -0.1241,\n",
       "          -0.9685,  0.2974,  0.2999, -0.1275,  0.4291,  0.6601, -0.4467, -0.2950,\n",
       "          -0.1629, -0.1969, -0.2722, -0.1901,  0.1047, -0.0980, -0.2156, -0.3158,\n",
       "           0.1827, -0.3681, -0.1540,  0.3989,  0.1522,  0.5510,  0.5563, -0.3650,\n",
       "           0.1886, -0.8343,  0.5148, -0.2391, -0.9441, -0.6315, -0.9636,  0.5626,\n",
       "           0.1190, -0.1677,  0.8216,  0.0150,  0.3511,  0.0218, -0.4375, -1.0000,\n",
       "          -0.3994, -0.1221, -0.0023, -0.0908, -0.9197, -0.8559,  0.5066,  0.8435,\n",
       "           0.1390,  0.9992, -0.1312,  0.8396, -0.0210, -0.4195, -0.1498, -0.4136,\n",
       "           0.4651, -0.1393, -0.3571,  0.1936, -0.1628,  0.1102, -0.3371, -0.2640,\n",
       "          -0.2312, -0.7335, -0.3424,  0.8618,  0.0971, -0.5568,  0.3000, -0.2271,\n",
       "          -0.2034,  0.7185,  0.2655,  0.2008, -0.1515,  0.3388,  0.1780,  0.3493,\n",
       "          -0.6862, -0.0101,  0.2526, -0.2629, -0.4194, -0.9355, -0.3440,  0.3791,\n",
       "           0.9288,  0.5353,  0.1655,  0.4088, -0.3730,  0.4544, -0.8915,  0.9356,\n",
       "           0.0316,  0.2175,  0.1174,  0.2893, -0.5349,  0.0381,  0.7299, -0.2590,\n",
       "          -0.6550,  0.0033, -0.4355, -0.4468, -0.3273,  0.3659, -0.1824, -0.3303,\n",
       "          -0.0479,  0.7960,  0.8812,  0.3969, -0.0928,  0.4448, -0.8017, -0.2502,\n",
       "           0.1422,  0.1704, -0.0562,  0.9674, -0.5023,  0.0736, -0.6019, -0.9507,\n",
       "           0.0233, -0.5868, -0.0502, -0.5345,  0.4382,  0.2432,  0.0849,  0.2056,\n",
       "          -0.7259, -0.5085,  0.1319, -0.3196,  0.3666, -0.2129,  0.6373,  0.6892,\n",
       "          -0.5651,  0.0153,  0.8974, -0.4952, -0.5085,  0.4938, -0.1404,  0.7362,\n",
       "          -0.5644,  0.9862,  0.5523,  0.2390, -0.7911, -0.4526, -0.5851, -0.3017,\n",
       "          -0.0366, -0.2206,  0.4366,  0.6444,  0.2364,  0.3800, -0.3827,  0.8778,\n",
       "          -0.3482, -0.8844, -0.3531, -0.0929, -0.9608,  0.3616,  0.2976, -0.0503,\n",
       "          -0.3464, -0.4415, -0.8039,  0.5850,  0.0576,  0.8968, -0.0595, -0.7529,\n",
       "          -0.4285, -0.7817, -0.1060, -0.0706,  0.1510, -0.0554, -0.7766,  0.4361,\n",
       "           0.4028,  0.3129, -0.3671,  0.9430,  1.0000,  0.9113,  0.7174,  0.5531,\n",
       "          -0.9932, -0.1165,  1.0000, -0.8466, -1.0000, -0.7768, -0.5570,  0.1248,\n",
       "          -1.0000, -0.1192,  0.0215, -0.6756,  0.1789,  0.9302,  0.8506, -1.0000,\n",
       "           0.8056,  0.7922, -0.7214,  0.3174, -0.2384,  0.9328,  0.1882,  0.5949,\n",
       "          -0.2337,  0.4222, -0.6810, -0.6984,  0.0971, -0.0512,  0.9674,  0.1983,\n",
       "          -0.7355, -0.7132, -0.0925,  0.1064, -0.2101, -0.8707, -0.2057, -0.2722,\n",
       "           0.4711,  0.0664,  0.1608, -0.5293,  0.1368, -0.1846,  0.2078,  0.6693,\n",
       "          -0.9070, -0.4051, -0.0448, -0.1493, -0.3521, -0.9343,  0.9247, -0.2199,\n",
       "           0.4837,  1.0000,  0.0771, -0.6576,  0.3054, -0.0068, -0.2351,  1.0000,\n",
       "           0.5802, -0.9455, -0.6512,  0.5114, -0.4292, -0.4430,  0.9993, -0.1219,\n",
       "          -0.1302,  0.1866,  0.9529, -0.9722,  0.9339, -0.7649, -0.8427,  0.8564,\n",
       "           0.7489, -0.2046, -0.5900,  0.1171, -0.5669,  0.2605, -0.7716,  0.4246,\n",
       "           0.3273, -0.0286,  0.6919, -0.3823, -0.5803,  0.3677, -0.2119, -0.1042,\n",
       "           0.5961,  0.3676, -0.1285,  0.0305, -0.2215, -0.1857, -0.9150,  0.2492,\n",
       "           1.0000, -0.1959, -0.1579, -0.3867,  0.0680, -0.2117,  0.3551,  0.4102,\n",
       "          -0.1756, -0.7042,  0.0075, -0.5931, -0.9692,  0.3122,  0.0807, -0.1979,\n",
       "           0.9993,  0.1640,  0.1954,  0.1487,  0.6380,  0.1134,  0.2169,  0.2615,\n",
       "           0.9052, -0.3244,  0.6549,  0.5270, -0.4262, -0.1863, -0.4987, -0.0033,\n",
       "          -0.8542,  0.0714, -0.8324,  0.8415,  0.5583,  0.2340,  0.2024,  0.3249,\n",
       "           1.0000, -0.5703,  0.4093,  0.2953,  0.3421, -0.9892, -0.4903, -0.3130,\n",
       "          -0.0170, -0.2317, -0.2011,  0.0079, -0.9127,  0.3496, -0.0710, -0.7770,\n",
       "          -0.9545, -0.1625,  0.4884,  0.0306, -0.7807, -0.4333, -0.4488,  0.2779,\n",
       "          -0.2911, -0.8089,  0.1849, -0.1659,  0.3632, -0.2287,  0.6077,  0.3641,\n",
       "           0.6405, -0.2182, -0.2991, -0.1436, -0.7167,  0.3146, -0.5738, -0.2056,\n",
       "          -0.1135,  1.0000,  0.0601,  0.5663,  0.3928,  0.2968, -0.2763,  0.1338,\n",
       "           0.6522,  0.1857, -0.3169, -0.1843,  0.4033, -0.3621,  0.5775, -0.0871,\n",
       "           0.4149,  0.5395,  0.3985,  0.2120,  0.0306,  0.0661,  0.9433, -0.2175,\n",
       "          -0.0831, -0.3106, -0.1194, -0.2831,  0.1416,  1.0000,  0.1669,  0.3888,\n",
       "          -0.9745, -0.3483, -0.8018,  0.9999,  0.5995, -0.6817,  0.4190,  0.3408,\n",
       "          -0.1069,  0.2416, -0.1843, -0.2011,  0.2169,  0.0946,  0.8539, -0.3705,\n",
       "          -0.8978, -0.5853,  0.3461, -0.8210,  0.9972, -0.5743, -0.1019, -0.1676,\n",
       "          -0.0140, -0.3172, -0.1049, -0.9295, -0.1248, -0.0493,  0.8931,  0.1241,\n",
       "          -0.6794, -0.8590,  0.2913,  0.4426, -0.2593, -0.7980,  0.8821, -0.8900,\n",
       "           0.2043,  1.0000,  0.3964,  0.0213,  0.2239, -0.1861,  0.2761,  0.1327,\n",
       "           0.3229, -0.7458, -0.1978, -0.1275,  0.2942, -0.0973, -0.3871,  0.3958,\n",
       "           0.1614, -0.5964, -0.5605, -0.0319,  0.3108,  0.5461, -0.2734, -0.0957,\n",
       "           0.0773, -0.0477, -0.8297, -0.3142, -0.2257, -0.9991,  0.5323, -1.0000,\n",
       "           0.0650,  0.0795, -0.1629,  0.6526,  0.1331,  0.1202, -0.2666, -0.2289,\n",
       "           0.5501,  0.6100, -0.0873,  0.0057, -0.2663,  0.1964, -0.0615,  0.1504,\n",
       "          -0.2414,  0.5305, -0.1676,  1.0000,  0.0522, -0.3153, -0.6251,  0.2661,\n",
       "          -0.1109,  1.0000, -0.5076, -0.8708,  0.2906, -0.4820, -0.5215,  0.1096,\n",
       "           0.0935, -0.7016, -0.4971,  0.8333,  0.5586, -0.6497,  0.2061, -0.3181,\n",
       "          -0.3347,  0.0471,  0.3996,  0.9593,  0.0753,  0.6357,  0.2275,  0.1554,\n",
       "           0.8579,  0.2682,  0.0398,  0.0716,  1.0000,  0.2697, -0.8612,  0.2525,\n",
       "          -0.8851, -0.2468, -0.8166,  0.2048,  0.1552,  0.6520, -0.1696,  0.8679,\n",
       "          -0.2998, -0.0775,  0.1531, -0.0544,  0.3656, -0.8456, -0.9655, -0.9591,\n",
       "           0.2911, -0.4374, -0.0233,  0.2328,  0.1467,  0.2608,  0.3207, -1.0000,\n",
       "           0.8420,  0.4209,  0.4601,  0.8482,  0.3225,  0.3625,  0.2423, -0.9500,\n",
       "          -0.5077, -0.3588, -0.3328,  0.5337,  0.5579,  0.4907,  0.2610, -0.2855,\n",
       "          -0.2294, -0.1922, -0.3041, -0.9666,  0.4185, -0.2859, -0.6551,  0.8761,\n",
       "          -0.0596, -0.0477,  0.2882, -0.5734,  0.7511,  0.7018,  0.0929, -0.0158,\n",
       "           0.4148,  0.7025,  0.8458,  0.9727, -0.5803,  0.5222, -0.2178,  0.2815,\n",
       "           0.4983, -0.8490,  0.1726,  0.1202, -0.2064,  0.2913, -0.2553, -0.7424,\n",
       "           0.5466, -0.3626,  0.4178, -0.3477,  0.1431, -0.3686, -0.1633, -0.5297,\n",
       "          -0.3669,  0.6839,  0.0534,  0.8352,  0.4841, -0.0457, -0.4132, -0.1910,\n",
       "          -0.3436, -0.8161,  0.6559,  0.0225, -0.2644,  0.1563, -0.1682,  0.8082,\n",
       "           0.0273, -0.2548, -0.2017, -0.6508,  0.6364, -0.4198, -0.4423, -0.2637,\n",
       "           0.5403,  0.2698,  0.9984, -0.3743, -0.4945, -0.2857, -0.1695,  0.2412,\n",
       "          -0.2638, -1.0000,  0.1229, -0.3749,  0.3388,  0.1365,  0.3773, -0.4339,\n",
       "          -0.8717, -0.1721,  0.1992,  0.1611, -0.4621, -0.4162,  0.6862,  0.0317,\n",
       "           0.4756,  0.6116, -0.1572,  0.0393,  0.6865, -0.4716, -0.5657,  0.6175]],\n",
       "        grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d0ae90a5d9321f207782076159de396e428d70e79fd71d362cfa3a86e561964"
  },
  "kernelspec": {
   "display_name": "Python [conda env:nn]",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
